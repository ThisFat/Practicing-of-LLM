{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 实现 LLM 的架构\n",
    "\n",
    "LLM（如GPT，即生成式预训练 Transformer，Generative Pretrained Transformer）是一种大型深度神经网络架构，设计用于逐词（或逐 token）生成新文本。然而，尽管模型规模庞大，其结构却并没有想象中那么复杂，因为模型的许多组件是重复的（后文将对此展开说明）。图 4.2 展示了一个类 GPT 的 LLM 的整体视图，并突出了其主要组成部分。\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-11_15-27-06.png)\n",
    "\n",
    "我们通过以下 Python 字典来定义小型 GPT-2 模型的配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ```vocab_size``` 指的是BPE分词器使用的50257个词汇的词表大小\n",
    "- ```context_length``` 指的是模型所能处理的最大输入token数\n",
    "- ```emb_dim``` 指的是嵌入维度，每个token转换为768维的向量\n",
    "- ```n_layers```指定模型中 Transformer 模块的层数，后续章节将对此详解。\n",
    "- ```drop_rate```表示 dropout 机制的强度（例如，0.1 表示丢弃 10% 的隐藏单元），用于防止过拟合，具体内容请回顾第 3 章。\n",
    "- ```qkv_bias``` 参数决定是否在多头注意力的查询、键和值的线性层中加入偏置向量。我们最初会禁用该选项，以遵循现代大语言模型的标准，之后在第 6 章加载 OpenAI 预训练的 GPT-2 权重时再重新考虑该设置。\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-11_15-35-42.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])      #A\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])                       #B\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len, = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):                                       #C\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):                                                     #D\n",
    "        return x\n",
    "\n",
    "class DummyLayerNorm(nn.Module):                                              #E\n",
    "    def __init__(self, normalized_shape, eps=1e-5):                           #F\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "#A 为 TransformerBlock 设置占位符\n",
    "#B 为 LayerNorm 设置占位符\n",
    "#C 一个简单的占位类，后续将被真正的 TransformerBlock 替换\n",
    "#D 该模块无实际操作，仅原样返回输入\n",
    "#E 一个简单的占位类，后续将被真正的 DummyLayerNorm 替换\n",
    "#F 此处的参数仅用于模拟LayerNorm接口\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这两段文本的token ID 如下：\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(f\"这两段文本的token ID 如下：\\n {batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 接下来，我们初始化一个拥有 1.24 亿参数的 DummyGPTModel 模型实例，并将分词后的数据批量输入到模型中：\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出的张量有两行，每行对应一段文本。每段文本包含 4 个 token，每个 token 是一个 50,257 维的向量，维度大小与分词器的词汇表相同。\n",
    "\n",
    "嵌入层的维度为 50,257，因为**每个维度**对应词汇表中的**一个唯一 token**。在之后的处理中，我们会将这些 50,257 维向量转换回 token ID，然后再解码成单词。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 使用层归一化(Layer norm)对激活值进行标准化\n",
    "\n",
    "在训练深度神经网络时，梯度消失或梯度爆炸问题有时会带来挑战。这些问题会导致训练过程不稳定，使得网络难以有效调整权重，也就是说，模型难以找到一组能最小化损失函数的参数。换句话说，模型很难从数据中学习到足够准确的模式，以支持其做出准确的预测或决策。\n",
    "\n",
    "> [!TIP]\n",
    ">\n",
    "> **个人思考：** 虽然对文本内容的理解并不需要深度掌握梯度的概念，但如果我们在学习过程中能习惯去发散，往往能帮助我们对所学知识理解的更深刻，下面我们就来聊一下梯度。\n",
    ">\n",
    "> 梯度本质上是一个**变化率**，描述了某个值（例如函数输出值）对另一个值（如输入变量）的变化趋势。简单来说，梯度告诉我们在当前位置上，朝哪个方向移动能让某个目标值增加或减少得更快。\n",
    ">\n",
    "> 举例：山坡上的爬山者\n",
    ">\n",
    "> 假设你站在一座山的某个位置，想要找到最快下山的路线。你会怎么做呢？首先你会注意到山坡的倾斜度（也就是梯度），倾斜越陡的地方，就意味着朝这个方向走可以让你更快地下降海拔。\n",
    ">\n",
    "> 在这个例子中：\n",
    ">\n",
    "> + **你的当前位置**代表模型当前的参数值。\n",
    "> + **山坡的倾斜度**就是梯度，表示你在当前位置向下走的快慢和方向。\n",
    "> + **往斜坡最陡的方向走**相当于使用梯度更新模型参数，使得海拔（也就是损失值）尽快下降。\n",
    ">\n",
    "> 而大模型在应用梯度的概念时，首先会设计一个损失函数，用来衡量模型的预测结果与目标结果的差距。在训练过程中，它通过梯度去帮助每个模型参数不断调整来快速减少损失函数的值，从而提高模型的预测精度。\n",
    "\n",
    "归一化的```核心思想```是将神经网络层的激活（输出）调整为均值为 0，方差为 1（即单位方差）。这种调整可以加速权重的收敛速度，确保训练过程的一致性和稳定性。正如上一节提到的，在 GPT-2 和现代 Transformer 架构中，层归一化通常应用于多头注意力模块的前后以及最终输出层之前。\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-11_17-06-07.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在计算均值或方差等操作时使用 keepdim=True 参数，可以确保输出张量的维度与输入张量相同，即使该操作通过dim参数减少了张量的维度。例如，如果不使用 keepdim=True，返回的均值张量将是一个二维向量 [0.1324, 0.2170]，而使用 keepdim=True 后，返回的张量则会是一个 2×1 的矩阵 [[0.1324], [0.2170]]\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-12_16-46-29.png)\n",
    "\n",
    "如图 4.6 所示，对于二维张量（如矩阵），在进行均值或方差计算等操作时，使用 dim=-1 等同于使用 dim=1，因为 -1 指的是张量的最后一个维度，即二维张量中的列。在后续对 GPT 模型加入层归一化时，模型会生成形状为 [batch_size, num_tokens, embedding_size] 的三维张量，我们依然可以使用 dim=-1 对最后一个维度进行归一化，而无需将 dim=1 改为 dim=2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim = -1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5     #epsilon, 防止出现除零错误\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))      #nn.Parameter, Tensor的subclass，\n",
    "                                                            #1.自动设置 requires_grad=True；2.被 nn.Module 识别为“可学习参数”\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim = True, unbiased = True)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-1.4901e-08],\n",
      "        [ 2.3842e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.8000],\n",
      "        [0.8000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 实现带有 GELU 激活函数的前馈神经网络\n",
    "\n",
    "GELU 和 SwiGLU 是更复杂、平滑的激活函数，分别结合了高斯分布和 sigmoid 门控线性单元。与较简单的 ReLU 不同，这些激活函数能为深度学习模型提供更好的性能。\n",
    "\n",
    "GELU 激活函数可以通过多种方式实现，其确切版本定义为 GELU(x) = x ⋅ Φ(x)，其中 Φ(x) 是标准正态分布的累积分布函数。然而在实践中，通常会采用计算开销更低的近似实现（最初的 GPT-2 模型也是用这种近似实现进行训练的）：\n",
    "\n",
    "$$ \\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot\\left(1+\\tanh \\left[\\sqrt{(2 / \\pi)} \\cdot\\left(x+0.044715 \\cdot x^{3}\\right]\\right)\\right. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#更直观的观察 GELU 函数的形状，并与 ReLU 函数进行对比，我们将这两个函数并排绘制：\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)                                          #A\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#A 在 -3 到 3 的范围内生成 100 个样本数据点"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```GELU``` 的**平滑性**使其在训练过程中具有更好的优化特性，能够对模型参数进行更细微的调整。相比之下，ReLU 在零点处有一个拐角，这在网络深度较大或结构复杂时可能会增加优化难度。此外，ReLU 对所有负输入的输出为零，而 GELU 对```负值```允许一个小的非零输出。这意味着在训练过程中，```接收负输入```的神经元也能对学习过程产生一定的贡献，尽管贡献程度不及正输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如代码所示，FeedForward 模块是一个小型神经网络，由两个线性层和一个 GELU 激活函数组成。在 1.24 亿参数的 GPT 模型中，该模块可以接收批量输入，每个输入 token 是一个 768 维的向量表示。这一嵌入维度大小通过 GPT_CONFIG_124M 配置字典中的 GPT_CONFIG_124M[\"emb_dim\"] 参数指定。\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-12_17-39-02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)          #A\n",
    "out = ffn(x)\n",
    "print(out.shape)\n",
    "\n",
    "#A 创建一个 batch 大小为 2 的示例输入"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!TIP]\n",
    "> 为什么要“扩展 (Upscale)”或“收缩 (Downscale)”嵌入维度？\n",
    "> \n",
    "> ```Upscale``` 的好处\n",
    "> | 角度           | 具体收益                                                           | 说明                                  |\n",
    ">| ------------ | -------------------------------------------------------------- | ----------------------------------- |\n",
    ">| **表达能力提升**   | 更容易捕获复杂语义、细粒度上下文或高阶共现信息                                        | 维度越高，可容纳的“正交方向”越多；模型能学到更细的概念分解或特征交互 |\n",
    ">| **线性可分性增强**  | 某些原本线性不可分的类别在高维空间往往可分                                          | “核技巧”直观：往高维投射==在原空间引入非线性            |\n",
    ">| **梯度信号更丰富**  | 在 Transformer、Deep & Cross Network 等架构里，宽阔的嵌入能缓解信息瓶颈，减小梯度消失/饱和 | 与“增宽隐藏层”作用类似                        |\n",
    ">| **多任务共享更灵活** | 同一个嵌入向量可同时服务多任务或多语言，维度大时任务间冲突更易被分割到不同子空间                       | 典型如 mBERT、M3E 等多语或多模态模型             |\n",
    "> \n",
    "> ```Downscale``` 的好处\n",
    "> | 角度           | 具体收益                         | 说明                            |\n",
    ">| ------------ | ---------------------------- | ----------------------------- |\n",
    ">| **内存与吞吐**    | 显存、带宽、IO 成本线性降低；推理时 QPS 提升显著 | 在大型召回/推荐系统中常用 16-32 维超小嵌入     |\n",
    ">| **正则化与泛化**   | 低维空间对噪声的拟合能力下降，相当于隐式正则       | “降维=减少可学习自由度”，可抑制过拟合、提升小样本鲁棒性 |\n",
    ">| **训练更快、更稳定** | 参数量少，梯度估计方差更低，收敛速度加快         | 对低资源任务、快速迭代原型尤其友好             |\n",
    ">| **便于部署/蒸馏**  | 模型文件更小，延迟与能耗下降，可在移动端/边缘设备运行  | 结合量化、剪枝可进一步压缩                 |\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-12_17-47-55.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 添加快捷连接\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-12_17-48-45.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码实现了一个 5 层的深度神经网络，每层包括一个线性层和 GELU 激活函数。在前向传播中，我们将输入逐层传递，同时如果 self.use_shortcut 属性设置为 True，则会添加图 4.12 所示的快捷连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123) # specify random seed for the initial weights for reproducibility\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们实现一个用于在模型反向传播过程中计算梯度的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()     \n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码中，我们定义了一个损失函数，用来计算模型输出与用户指定目标（此处为简单起见，目标值设为 0）之间的差距。接着，当调用 loss.backward() 时，PyTorch 会为模型的每一层计算损失的梯度。我们可以通过 model.named_parameters() 遍历权重参数。假设某层的权重参数是一个 3×3 的矩阵，那么这一层会有 3×3 的梯度值。然后我们打印出这 3×3 梯度值的绝对均值，以便得到每层的单一梯度值，从而更容易比较各层之间的梯度大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 print_gradients 函数的输出可以看出，梯度在从最后一层（layers.4）到第一层（layers.0）时逐渐减小，这种现象称为梯度消失问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.2216978669166565\n",
      "layers.1.0.weight has gradient mean of 0.20694100856781006\n",
      "layers.2.0.weight has gradient mean of 0.3289698660373688\n",
      "layers.3.0.weight has gradient mean of 0.2665731906890869\n",
      "layers.4.0.weight has gradient mean of 1.3258538246154785\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总之，快捷连接在解决深度神经网络中的梯度消失问题方面具有重要作用。作为 LLM 的核心构建单元，快捷连接可以确保各层之间的梯度稳定流动，从而帮助 GPT 模型更有效的训练（下一章实现训练过程）\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 在 Transformer 模块中连接注意力层与线性层\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-14_13-51-22.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Self-Attention 结果 (含位置编码) =====\n",
      "\n",
      "       x1        x2       x3       T1       T2       T3       T4       T5       T6       o1       o2       o3\n",
      " 1.271471  0.690302 0.890005 0.315381 0.239435 0.105702 0.044346 0.066093 0.229044 1.273928 1.060221 0.435727\n",
      " 1.459297  0.453853 0.660009 0.296198 0.233471 0.115046 0.054239 0.076627 0.224420 1.236005 1.028942 0.420495\n",
      " 0.711120 -0.139992 0.640014 0.231497 0.205499 0.144342 0.099181 0.117838 0.201644 1.086208 0.905413 0.360154\n",
      "-0.536803 -0.073644 0.330019 0.162206 0.163468 0.167715 0.172386 0.170190 0.164035 0.885203 0.739702 0.278801\n",
      "-0.188924  0.533662 0.100023 0.193902 0.184244 0.158497 0.135087 0.145366 0.182904 0.982739 0.820102 0.318358\n",
      "-0.229415  1.760170 0.550028 0.288950 0.230573 0.118408 0.058350 0.080746 0.222973 1.221021 1.016597 0.414371\n",
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n",
      "tensor([0.4419, 0.6515, 0.5683])\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "query_2 is tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>), \n",
      "key_2 is tensor([0.4433, 1.1419], grad_fn=<SqueezeBackward4>), \n",
      "value_2 is tensor([0.3951, 1.0037], grad_fn=<SqueezeBackward4>)\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "tensor(1.8524, grad_fn=<DotBackward0>)\n",
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)\n",
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
      " Linear(in_features=3, out_features=2, bias=False)\n",
      "\n",
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n",
      "Difference: tensor(0., grad_fn=<MaxBackward1>)\n",
      "attn_weights are: \n",
      "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
      "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
      "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "context_length is 6\n",
      "\n",
      "mask_simple is \n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "hence the final masked_simple is \n",
      "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "masked: \n",
      "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weights: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 6, 3])\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n",
      "6\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n",
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n",
      "output shape is: torch.Size([2, 1024, 768]) \n",
      "只看第一个样本、前三个 token，以及每个 token 的前 8 个维度: \n",
      "tensor([[ 0.5392, -0.5615,  0.0447, -0.1724, -0.0308,  0.2525,  0.2441,  0.1450],\n",
      "        [ 0.0109, -0.2011,  0.1073,  0.0025, -0.2709,  0.2415,  0.0765,  0.2479],\n",
      "        [-0.1092, -0.1441,  0.0484, -0.0340, -0.0342,  0.1451, -0.1512,  0.2373]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "%run ./3_Coding_Attention_Mechanisms.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "        d_in=cfg[\"emb_dim\"],\n",
    "        d_out=cfg[\"emb_dim\"],\n",
    "        context_length=cfg[\"context_length\"],\n",
    "        num_heads=cfg[\"n_heads\"],\n",
    "        dropout=cfg[\"drop_rate\"],\n",
    "        qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg=cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层归一化（LayerNorm）在这两个组件（即自注意力和前馈网络）之前应用，而 dropout 则在它们之后应用，用于正则化模型并防止过拟合。这种方式也称为前置层归一化（Pre-LayerNorm）。而在早期的架构中（如原始的 Transformer 模型），一般将层归一化应用在自注意力和前馈网络之后，这被称为后置层归一化（Post-LayerNorm），这种方式通常会导致较差的训练效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)                    #A\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "\n",
    "#A 建一个形状为 [batch_size, num_tokens, emb_dim] 的输入张量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 模块结构中保持数据形状不变并非偶然，而是其设计的一个关键特性。这种设计使 Transformer 擅长处理各种序列到序列任务，因为每个输出向量直接对应一个输入向量，保持一一对应关系。然而，虽然维度一致，但输出向量是包含整个输入序列信息的“上下文向量”。也就是说，尽管序列的物理维度（长度和特征维度）在经过 Transformer 模块时保持不变，但每个输出向量的内容会被重新编码，融合整个输入序列的上下文信息。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 实现 GPT 模型\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-14_15-42-01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.7 The GPT model architecture implementation\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))      #A\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    " #A 设备设置将根据输入数据所在的位置选择在 CPU 或 GPU 上训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0079, -0.1957,  ..., -0.0222, -0.1062,  0.1717],\n",
      "         [ 0.3867, -0.8400, -0.6558,  ..., -0.5162,  0.2362, -0.3349],\n",
      "         [ 0.6985, -0.1826, -0.1634,  ...,  0.1472, -0.6503, -0.0054],\n",
      "         [-0.4288,  0.1670, -0.1262,  ...,  1.1571,  0.5297, -0.5542]],\n",
      "\n",
      "        [[ 0.1095, -0.2890, -0.1463,  ..., -0.0557,  0.2907, -0.2818],\n",
      "         [ 0.0884, -0.3545, -0.3524,  ...,  1.2921,  0.0050,  0.1902],\n",
      "         [ 0.6092,  0.4702, -0.4093,  ...,  0.7682,  0.3781, -0.1968],\n",
      "         [-0.0608, -0.0739,  0.4747,  ...,  1.2458, -0.3834,  0.0612]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，输出张量的形状是 [2, 4, 50257]，这是因为我们输入了 2 个文本，每个文本包含 4 个 token。最后一个维度 50257 对应于分词器的词汇表大小。在下一节中，我们将看到如何将这些 50257 维的输出向量转换回 token。\n",
    "\n",
    "使用 numel() 方法（即 number of elements 的缩写），可以统计模型中参数张量的总参数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们之前提到 GPT 模型的参数量为 1.24 亿，但代码输出的实际参数量却是 1.63 亿，这是为什么呢？\n",
    "\n",
    "原因在于 GPT-2 架构中使用了一种称为‘权重共享’的概念，这意味着 GPT-2 架构将 token 嵌入层的权重复用于输出层。\n",
    "\n",
    "token 嵌入层和输出层的参数量很大，因为分词器词汇表中包含 50,257 个 token。根据权重共享原则，我们可以从 GPT-2 模型的总参数量中去除输出层的参数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n",
      "\n",
      "-------------------\n",
      "\n",
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "\n",
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(\"\\n-------------------\")\n",
    "print(f\"\\nNumber of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA number of parameters: 28,320,768\n",
      "FFN number of parameters:  56,669,184\n"
     ]
    }
   ],
   "source": [
    "def count_mha_ffn(model):\n",
    "    mha, ffn = 0, 0\n",
    "    for block in model.trf_blocks:          # 逐层\n",
    "        mha += sum(p.numel() for p in block.att.parameters() if p.requires_grad)\n",
    "        ffn += sum(p.numel() for p in block.ff.parameters()  if p.requires_grad)\n",
    "    return mha, ffn\n",
    "\n",
    "mha, ffn = count_mha_ffn(model)\n",
    "print(f\"MHA number of parameters: {mha:,}\")\n",
    "print(f\"FFN number of parameters:  {ffn:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4                 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)    #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.3184,  0.2476, -0.0316,  ..., -0.0517, -0.0864, -0.3170],\n",
      "         [ 0.1490, -0.2821, -0.4085,  ..., -0.1722,  0.5008,  0.3073],\n",
      "         [ 0.5052, -0.2710,  0.4978,  ..., -0.1405, -0.6055, -0.2000],\n",
      "         [-1.3377, -0.0232, -0.0723,  ...,  1.3942,  0.2077, -0.7040]],\n",
      "\n",
      "        [[-0.0803, -0.3320, -0.0305,  ...,  0.0070, -0.1439, -1.1663],\n",
      "         [ 0.0586,  0.0853,  0.1376,  ...,  0.4377,  0.0424,  0.7175],\n",
      "         [ 0.9623,  0.5480,  0.1374,  ...,  0.6531, -0.2354,  0.1913],\n",
      "         [-0.0499,  0.3240,  0.4199,  ...,  1.4464,  0.0243, -0.0925]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "GPT_CONFIG_medium = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 25,          # Number of attention heads\n",
    "    \"n_layers\": 48,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "model_medium = GPTModel(GPT_CONFIG_medium)\n",
    "out_medium = model_medium(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out_medium.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 1,637,792,000\n",
      "\n",
      "-------------------\n",
      "\n",
      "Number of trainable parameters considering weight tying: 1,557,380,800\n"
     ]
    }
   ],
   "source": [
    "total_params_medium = sum(p.numel() for p in model_medium.parameters())\n",
    "print(f\"Total number of parameters: {total_params_medium:,}\")\n",
    "total_params_gpt2_medium = total_params_medium - sum(p.numel() for p in model_medium.out_head.parameters())\n",
    "print(\"\\n-------------------\")\n",
    "print(f\"\\nNumber of trainable parameters considering weight tying: {total_params_gpt2_medium:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 生成文本\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-14_16-20-59.png)\n",
    "\n",
    "如图 4.16 所示，GPT 模型在给定输入上下文（例如 ‘Hello, I am’）后，逐步生成文本。每次迭代中，输入上下文会不断扩展，使模型能够生成连贯且符合上下文的内容。在第 6 次迭代时，模型已构建出完整句子 ‘Hello, I am a model ready to help.’。\n",
    "\n",
    "在上一节，我们看到目前的 GPTModel 输出的张量形状为 [batch_size, num_token, vocab_size]。那么问题来了，GPT 模型是如何将这些输出张量转化为图 4.16 所示的生成文本的呢？\n",
    "\n",
    "GPT 模型从输出张量到生成文本的过程涉及几个步骤（如图 4.17 所示）。这些步骤包括解码输出张量、根据概率分布选择 token，并将其转化为可读文本。\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-14_16-22-15.png)\n",
    "\n",
    "在每一步，模型会输出一个矩阵，其中的向量表示潜在的下一个 token。取出对应于下一个 token 的向量，并通过 softmax 函数将其转换为概率分布。在包含概率分数的向量中，找到最高值的索引，并将其转换为 token ID。将该 token ID 解码回文本，得到序列中的下一个 token。最后，将该 token 添加到先前的输入中，形成下一次迭代的新输入序列。这种逐步生成的过程使模型能够根据初始输入上下文，按顺序生成文本，从而构建出连贯的短语和句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 4.8 A function for the GPT model to generate text\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size): #A\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]                           #B\n",
    "        with torch.no_grad():\n",
    "           logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]                                   #C\n",
    "        probas = torch.softmax(logits, dim=-1)                      #D\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)       #E\n",
    "        idx = torch.cat((idx, idx_next), dim=1)                     #F\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "#A idx 是当前上下文中索引的数组，形状为 (batch, n_tokens)\n",
    "#B 若上下文长度超出支持范围，则进行裁剪。例如，若模型仅支持 5 个 token，而上下文长度为 10，仅使用最后 5 个 token 作为上下文\n",
    "#C 仅关注最后一个时间步，将形状从 (batch, n_token, vocab_size) 转换为 (batch, vocab_size)\n",
    "#D probas 的形状为 (batch, vocab_size)\n",
    "#E idx_next 的形状为 (batch, 1)\n",
    "#F 将采样的索引追加到当前序列中，此时 idx 的形状为 (batch, n_tokens+1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上述代码中，generate_text_simple 函数使用 Softmax 函数将 logits 转换为概率分布，然后通过 torch.argmax 找出概率最高的位置。Softmax 函数是单调的，这意味着它会保持输入的相对顺序，因此，Softmax 这一步实际上是冗余的，因为 Softmax 输出中最高值的位置与原始 logits 中最高值的位置相同。换句话说，我们可以直接对 logits 应用 torch.argmax 得到相同的结果。不过，我们保留了这个转换过程，以展示从 logits 到概率的完整过程，有助于理解模型如何生成最可能的下一个词，这种方式称为贪婪解码。\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-14_16-36-45.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_porj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(f\"encoded: {encoded}\")\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)            #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n",
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
