{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 张量是用于存储类似数组结构的数据容器。标量是 0 维张量（例如，一个简单的数字），向量是 1 维张量，而矩阵是 2 维张量。对于更高维度的张量没有特定的术语，所以我们通常将 3 维张量称为 3D 张量，以此类推。\n",
    "\n",
    "我们可以使用 torch.tensor 函数创建 PyTorch 的 Tensor 类的对象，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing A.1 Creating PyTorch tensors\n",
    "\n",
    "tensor0d = torch.tensor(1)                                    #A\n",
    "tensor1d = torch.tensor([1, 2, 3])                            #B\n",
    "tensor2d = torch.tensor([[1, 2], [3, 4]])                     #C\n",
    "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) #D\n",
    "\n",
    "\n",
    "\n",
    "#A 从 Python 整数创建一个 0 维张量（标量）\n",
    "#B 从 Python 列表创建一个 1 维张量（向量）\n",
    "#C 从嵌套的 Python 列表创建一个 2 维张量\n",
    "#D 从嵌套的 Python 列表创建一个 3 维张量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Tensor的数据类型中，主流的是什么，为什么不选择浮点数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.int64\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(tensor0d.dtype)\n",
    "print(tensor1d.dtype)\n",
    "print(tensor2d.dtype)\n",
    "print(tensor3d.dtype)\n",
    "\n",
    "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(floatvec.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种选择主要是基于精度和计算效率之间的平衡。对于大多数深度学习任务来说，32 位浮点数提供了足够的精度，同时比 64 位浮点数消耗更少的内存和计算资源。此外，GPU 架构针对 32 位计算进行了优化，使用这种数据类型可以显著加快模型训练和推理的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor(1)\n",
      "torch.Size([])\n",
      "--------\n",
      "tensor([1, 2, 3])\n",
      "torch.Size([3])\n",
      "--------\n",
      "tensor([1., 2., 3.])\n",
      "torch.Size([3])\n",
      "--------\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "torch.Size([2, 2])\n",
      "--------\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "torch.Size([2, 2, 2])\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "float_tensor_1d = tensor1d.to(torch.float32)\n",
    "print(float_tensor_1d.dtype)\n",
    "\n",
    "print(tensor0d)\n",
    "print(tensor0d.shape)\n",
    "print(\"--------\")\n",
    "print(tensor1d)\n",
    "print(tensor1d.shape)\n",
    "print(\"--------\")\n",
    "print(float_tensor_1d)\n",
    "print(float_tensor_1d.shape)\n",
    "print(\"--------\")\n",
    "print(tensor2d)\n",
    "print(tensor2d.shape)\n",
    "print(\"--------\")\n",
    "print(tensor3d)\n",
    "print(tensor3d.shape)\n",
    "print(\"--------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个具体的例子展示简单逻辑回归分类器的前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0852)\n"
     ]
    }
   ],
   "source": [
    "# Listing A.2 A logistic regression forward pass\n",
    "\n",
    "import torch.nn.functional as F #A\n",
    "\n",
    "y = torch.tensor([1.0])         #B\n",
    "x1 = torch.tensor([1.1])        #C\n",
    "w1 = torch.tensor([2.2])        #D\n",
    "b = torch.tensor([0.0])         #E\n",
    "z = x1 * w1 + b                 #F\n",
    "a = torch.sigmoid(z)            #G\n",
    "\n",
    "loss = F.binary_cross_entropy(a, y)\n",
    "print(loss)\n",
    "\n",
    "#A 这是 PyTorch 中常见的导入约定，用于避免代码行过长\n",
    "#B 真实标签\n",
    "#C 输入特征\n",
    "#D 权重参数\n",
    "#E 偏置单元\n",
    "#F 网络输入\n",
    "#G 激活与输出"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __init__ 构造函数中定义网络层，并在 forward 方法中指定它们如何交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "# Listing A.4 A multilayer perceptron with two hidden layers\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):          #A\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_inputs, 30),              #B\n",
    "            torch.nn.ReLU(),                              #C\n",
    "\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(30, 20),                      #D\n",
    "            torch.nn.ReLU(),\n",
    "\n",
    "            # output layer\n",
    "            torch.nn.Linear(20, num_outputs),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits                                     #E\n",
    "      \n",
    "\n",
    "#A 将输入和输出的数量编码为变量很有用，这样可以为具有不同特征和类别数量的数据集重用相同的代码。\n",
    "#B Linear 层将输入和输出节点的数量作为参数。\n",
    "#C 非线性激活函数放置在隐藏层之间。\n",
    "#D 一个隐藏层的输出节点数必须与下一个隐藏层的输入节点数相匹配。\n",
    "#E 最后一层的输出被称为 logits。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=50, out_features=30, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(50,3)\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，在实现 NeuralNetwork 类时，我们使用了 Sequential 类。使用 Sequential 不是必需的，但如果我们有一系列想要按特定顺序执行的层（就像这里的情况一样），它可以使我们的工作更轻松。这样，在 __init__ 构造函数中实例化 self.layers = Sequential(...) 之后，我们只需要调用 self.layers，而无需在 NeuralNetwork 的 forward 方法中单独调用每个层。\n",
    "\n",
    "请注意，每个 requires_grad=True 的参数都被认为是可训练参数，并且将在训练期间更新（更多内容请参见 2.7 节“一个典型的训练循环”）。\n",
    "对于我们上面定义的具有两个隐藏层的神经网络模型，这些可训练参数包含在 torch.nn.Linear 层中。一个线性层将输入与权重矩阵相乘，并加上一个偏置向量。这有时也被称为前馈层或全连接层。\n",
    "根据我们上面执行的 print(model) 调用，我们可以看到第一个 Linear 层位于 layers 属性的索引位置 0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0250, -0.0269,  0.0146,  ...,  0.0749, -0.0085, -0.0170],\n",
      "        [-0.1405, -0.0268,  0.1258,  ..., -0.0536, -0.0714,  0.0032],\n",
      "        [-0.0794,  0.0754, -0.0627,  ...,  0.1195,  0.1399, -0.1105],\n",
      "        ...,\n",
      "        [ 0.1389,  0.0277,  0.0631,  ..., -0.0038, -0.1367,  0.0272],\n",
      "        [-0.0591, -0.1273,  0.0071,  ...,  0.0970,  0.0499,  0.0907],\n",
      "        [ 0.0583, -0.1370, -0.1158,  ...,  0.0260, -0.0522, -0.0721]],\n",
      "       requires_grad=True)\n",
      "torch.Size([30, 50])\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[0].weight)\n",
    "print(model.layers[0].weight.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些权重的初始化是随机的，因此需要使用种子来固定这些随机化的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0577,  0.0047, -0.0702,  ...,  0.0222,  0.1260,  0.0865],\n",
      "        [ 0.0502,  0.0307,  0.0333,  ...,  0.0951,  0.1134, -0.0297],\n",
      "        [ 0.1077, -0.1108,  0.0122,  ...,  0.0108, -0.1049, -0.1063],\n",
      "        ...,\n",
      "        [-0.0787,  0.1259,  0.0803,  ...,  0.1218,  0.1303, -0.1351],\n",
      "        [ 0.1359,  0.0175, -0.0673,  ...,  0.0674,  0.0676,  0.1058],\n",
      "        [ 0.0790,  0.1343, -0.0293,  ...,  0.0344, -0.0971, -0.0509]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = NeuralNetwork(50, 3)\n",
    "print(model.layers[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing A.5 Creating a small toy dataset\n",
    "\n",
    "X_train = torch.tensor([\n",
    "    [-1.2, 3.1],\n",
    "    [-0.9, 2.9],\n",
    "    [-0.5, 2.6],\n",
    "    [2.3, -1.1],\n",
    "    [2.7, -1.5]\n",
    "])\n",
    "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
    "\n",
    "X_test = torch.tensor([\n",
    "    [-0.8, 2.8],\n",
    "    [2.6, -1.6],\n",
    "])\n",
    "y_test = torch.tensor([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        one_x = self.features[index]\n",
    "        one_y = self.labels[index]\n",
    "        return one_x,one_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "train_dataset = ToyDataset(X_train,y_train)\n",
    "test_dataset = ToyDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经为我们的数据集定义了一个 PyTorch Dataset 类，接着可以使用 PyTorch 的 DataLoader 类来从中采样数据，如下面的代码清单所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: tensor([[ 2.3000, -1.1000],\n",
      "        [-0.9000,  2.9000]]) tensor([1, 0])\n",
      "Batch 2: tensor([[-1.2000,  3.1000],\n",
      "        [-0.5000,  2.6000]]) tensor([0, 0])\n",
      "Batch 3: tensor([[ 2.7000, -1.5000]]) tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Listing A.7 Instantiating data loaders\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,       #A\n",
    "    batch_size=2,\n",
    "    shuffle=True,           #B\n",
    "    num_workers=0           #C\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,          #D\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "\n",
    "#A 之前创建的 ToyDataset 实例作为数据加载器的输入。\n",
    "#B 是否打乱数据\n",
    "#C 后台进程的数量\n",
    "#D 没有必要打乱测试数据\n",
    "for idx, (x, y) in enumerate(train_loader):\n",
    "\t\tprint(f\"Batch {idx+1}:\", x, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么要shuffle：因为如果不打乱顺序，模型很可能只记住了这些数据之间的顺序关系，而无法真正理解内容。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
