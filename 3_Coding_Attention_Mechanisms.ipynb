{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†\n",
    "\n",
    "![from comp5046](imgs/PixPin_2025-07-06_04-33-57.png)\n",
    "\n",
    "**Self-Attention**: A weighted average of vectors where the weights and input vectors are all from the input\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-06_04-36-28.png)\n",
    "\n",
    "å›¾ 3.7 æ˜¾ç¤ºäº†ä¸€ä¸ªè¾“å…¥åºåˆ—ï¼Œè®°ä½œ xï¼Œç”± T ä¸ªå…ƒç´ ç»„æˆï¼Œè¡¨ç¤ºä¸º x(1) åˆ° x(T)ã€‚è¯¥åºåˆ—é€šå¸¸ä»£è¡¨æ–‡æœ¬ï¼Œä¾‹å¦‚ä¸€ä¸ªå¥å­ï¼Œå¹¶ä¸”è¯¥æ–‡æœ¬å·²è¢«è½¬æ¢ä¸º token åµŒå…¥ã€‚\n",
    "\n",
    "ä¸¾ä¾‹æ¥è¯´ï¼Œå‡è®¾è¾“å…¥æ–‡æœ¬ä¸º â€œYour journey starts with one stepâ€ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œåºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼ˆå¦‚ x(1)ï¼‰å¯¹åº”ä¸€ä¸ª d ç»´çš„åµŒå…¥å‘é‡ï¼Œç”¨äºè¡¨ç¤ºç‰¹å®šçš„ tokenï¼Œä¾‹å¦‚ â€œYourâ€ã€‚åœ¨å›¾ 3.7 ä¸­ï¼Œè¿™äº›è¾“å…¥å‘é‡æ˜¾ç¤ºä¸º 3 ç»´çš„åµŒå…¥å‘é‡ã€‚\n",
    "\n",
    "åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæˆ‘ä»¬çš„**ç›®æ ‡**æ˜¯ä¸ºè¾“å…¥åºåˆ—ä¸­çš„æ¯ä¸ªå…ƒç´  x(i) è®¡ç®—å…¶å¯¹åº”çš„ä¸Šä¸‹æ–‡å‘é‡ z(i) ã€‚ä¸ºäº†è¯´æ˜è¿™ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬èšç„¦äºç¬¬äºŒä¸ªè¾“å…¥å…ƒç´  x(2) çš„åµŒå…¥å‘é‡ï¼ˆå¯¹åº”äºè¯ \"journey\"ï¼‰ä»¥åŠç›¸åº”çš„ä¸Šä¸‹æ–‡å‘é‡ z(2)ï¼Œå¦‚å›¾ 3.7 åº•éƒ¨æ‰€ç¤ºã€‚è¿™ä¸ªå¢å¼ºçš„ä¸Šä¸‹æ–‡å‘é‡ z(2) ä¹Ÿæ˜¯ä¸€ä¸ªåµŒå…¥å‘é‡ï¼ŒåŒ…å«äº†å…³äº x(2) ä»¥åŠåºåˆ—ä¸­æ‰€æœ‰å…¶ä»–è¾“å…¥å…ƒç´  x(1) åˆ° x(T) çš„è¯­ä¹‰ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imgs/PixPin_2025-07-06_04-40-41.png)\n",
    "\n",
    "è¿™é‡Œqiæ˜¯Gelatoï¼Œkjå¯ä»¥æ˜¯isæˆ–è€…deliciousã€‚ä½†è¿˜æœ‰é—®é¢˜ï¼Œé¦–å…ˆweightedæ˜¯çº¿æ€§çš„ï¼Œå¹¶ä¸”è¿™é‡Œå¹¶æ²¡æœ‰è¯åºçš„å…³ç³»,å› æ­¤æˆ‘ä»¬å¼•å…¥äº†ä¹‹å‰æåˆ°çš„ä½ç½®ç¼–ç ã€‚\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-06_04-41-22.png)\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-06_04-42-44.png)\n",
    "\n",
    "å®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç¬¬ä¸€æ­¥æ˜¯è®¡ç®—ä¸­é—´å€¼ Ï‰ï¼Œå³æ³¨æ„åŠ›å¾—åˆ†ï¼Œå¦‚å›¾ 3.8 æ‰€ç¤ºã€‚ï¼ˆè¯·æ³¨æ„ï¼Œå›¾ 3.8 ä¸­å±•ç¤ºçš„è¾“å…¥å¼ é‡å€¼æ˜¯æˆªæ–­ç‰ˆçš„ï¼Œä¾‹å¦‚ï¼Œç”±äºç©ºé—´é™åˆ¶ï¼Œ0.87 è¢«æˆªæ–­ä¸º 0.8ã€‚åœ¨æ­¤æˆªæ–­ç‰ˆä¸­ï¼Œå•è¯ \"journey\" å’Œ \"starts\" çš„åµŒå…¥å‘é‡å¯èƒ½ä¼šç”±äºéšæœºå› ç´ è€Œçœ‹èµ·æ¥ç›¸ä¼¼ï¼‰ã€‚\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-06_04-43-28.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ ¹æ®inputï¼Œæˆ‘ä»¬å¯ä»¥è®¾è®¡ä¸€ä¸ªå°demoï¼š\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬è¦åˆ¶å®šQï¼ŒKï¼ŒVçš„æƒé‡çŸ©é˜µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = torch.tensor([[0.1, 0.3, 0.5],  # 3x3\n",
    "                    [0.2, 0.4, 0.6],\n",
    "                    [0.3, 0.5, 0.7]])\n",
    "\n",
    "W_K = torch.tensor([[0.2, 0.1, 0.4],\n",
    "                    [0.3, 0.2, 0.5],\n",
    "                    [0.4, 0.3, 0.6]])\n",
    "\n",
    "W_V = torch.tensor([[0.5, 0.4, 0.3],\n",
    "                    [0.6, 0.5, 0.2],\n",
    "                    [0.7, 0.6, 0.1]])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åï¼Œè®¡ç®—ç»å¯¹ä½ç½®ç¼–ç ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def positional_encoding(pos, d_model):\n",
    "    pe = torch.zeros(d_model)\n",
    "    for i in range(0, d_model, 2):\n",
    "        angle = torch.tensor(pos / (10000 ** ((2 * i)/d_model)))\n",
    "        pe[i] = torch.sin(angle)\n",
    "\n",
    "        if i + 1 < d_model:\n",
    "            angle = torch.tensor(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            pe[i+1] = torch.cos(angle)\n",
    "    return pe\n",
    "    \n",
    "pos_encodings = torch.stack([positional_encoding(pos, inputs.shape[1]) for pos in range(1, 7)])\n",
    "\n",
    "inputs_with_pe = inputs + pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Self-Attention ç»“æœ (å«ä½ç½®ç¼–ç ) =====\n",
      "\n",
      "       x1        x2       x3       T1       T2       T3       T4       T5       T6       o1       o2       o3\n",
      " 1.271471  0.690302 0.890005 0.315381 0.239435 0.105702 0.044346 0.066093 0.229044 1.273928 1.060221 0.435727\n",
      " 1.459297  0.453853 0.660009 0.296198 0.233471 0.115046 0.054239 0.076627 0.224420 1.236005 1.028942 0.420495\n",
      " 0.711120 -0.139992 0.640014 0.231497 0.205499 0.144342 0.099181 0.117838 0.201644 1.086208 0.905413 0.360154\n",
      "-0.536803 -0.073644 0.330019 0.162206 0.163468 0.167715 0.172386 0.170190 0.164035 0.885203 0.739702 0.278801\n",
      "-0.188924  0.533662 0.100023 0.193902 0.184244 0.158497 0.135087 0.145366 0.182904 0.982739 0.820102 0.318358\n",
      "-0.229415  1.760170 0.550028 0.288950 0.230573 0.118408 0.058350 0.080746 0.222973 1.221021 1.016597 0.414371\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®— Q, K, V\n",
    "Q = inputs_with_pe @ W_Q  # (6,3)\n",
    "K = inputs_with_pe @ W_K  # (6,3)\n",
    "V = inputs_with_pe @ W_V  # (6,3)\n",
    "\n",
    "# è®¡ç®— attention scores (Q @ K.T / sqrt(d_k))\n",
    "d_k = Q.shape[1]\n",
    "scores = Q @ K.T / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "# Softmax å½’ä¸€åŒ–\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "# åŠ æƒæ±‚å’Œ\n",
    "output = attention_weights @ V\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# æ‹¼æ¥è¾“å…¥ã€æ³¨æ„åŠ›æƒé‡ã€è¾“å‡ºç»“æœ\n",
    "df_inputs = pd.DataFrame(inputs_with_pe.numpy(), columns=['x1', 'x2', 'x3'])\n",
    "df_attention = pd.DataFrame(attention_weights.numpy(), columns=[f\"T{i+1}\" for i in range(6)])\n",
    "df_output = pd.DataFrame(output.numpy(), columns=['o1', 'o2', 'o3'])\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰ DataFrame\n",
    "result_df = pd.concat([df_inputs, df_attention, df_output], axis=1)\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "print(\"\\n===== Self-Attention ç»“æœ (å«ä½ç½®ç¼–ç ) =====\\n\")\n",
    "print(result_df.to_string(index=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "```\n",
    "\n",
    "```x1, x2, x3```æ˜¯åŸå§‹è¯å‘é‡ + ç»å¯¹ä½ç½®ç¼–ç ï¼Œå³ inputs_with_pe\n",
    "\n",
    "```xml\n",
    "åŸå§‹å‘é‡: [0.43, 0.15, 0.89]\n",
    "ä½ç½®ç¼–ç : [0.84, 0.54, 0.00]\n",
    "ç›¸åŠ ç»“æœ: [1.271471, 0.690302, 0.89005]\n",
    "```\n",
    "\n",
    "```T1~T6```: æ³¨æ„åŠ›æƒé‡çŸ©é˜µçš„æŸä¸€è¡Œ,è¡¨ç¤ºå½“å‰ token å¯¹å¥å­ä¸­ æ¯ä¸ª token çš„å…³æ³¨åº¦ã€‚\n",
    "\n",
    "ä¾‹å­ï¼ˆç¬¬ 1 ä¸ª token çš„ attention åˆ†å¸ƒï¼‰ï¼š\n",
    "\n",
    "```md\n",
    "T1=0.31581  (è‡ªå·±: 31.6%)\n",
    "T2=0.239435 (å…³æ³¨ token2: 23.9%)\n",
    "...\n",
    "T6=0.229044 (å…³æ³¨ token6: 22.9%)\n",
    "```\n",
    "\n",
    "```o1~o3```: Self-Attention çš„æœ€ç»ˆè¾“å‡ºå‘é‡, å³æ¯ä¸ª token çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥è¡¨ç¤ºã€‚\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›åˆ°æ•™æï¼šå›¾ 3.8 å±•ç¤ºäº†å¦‚ä½•è®¡ç®—æŸ¥è¯¢ token ä¸æ¯ä¸ªè¾“å…¥ token ä¹‹é—´çš„ä¸­é—´æ³¨æ„åŠ›å¾—åˆ†ã€‚æˆ‘ä»¬é€šè¿‡è®¡ç®—æŸ¥è¯¢ x(2) ä¸æ¯ä¸ªå…¶ä»–è¾“å…¥ token çš„ç‚¹ç§¯æ¥ç¡®å®šè¿™äº›å¾—åˆ†ï¼ˆè¿™é‡Œåªæ˜¯ç®€å•çš„å°†ç¬¬äºŒä¸ªtokenä½œä¸ºquery vectorè¿›è¡Œäº†qä¸kçš„dot productï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]                                               #A\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)\n",
    "\n",
    "\n",
    "#A ç¬¬äºŒä¸ªè¾“å…¥ token ç”¨ä½œæŸ¥è¯¢å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ softmax å‡½æ•°æ¥è¿›è¡Œå½’ä¸€åŒ–ã€‚\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> 1. Softmax çš„å¥½å¤„\n",
    "> \n",
    "> - ```å½’ä¸€åŒ–è¾“å‡ºä¸ºæ¦‚ç‡```ï¼šSoftmax å°†è¾“å‡ºè½¬æ¢ä¸º 0 åˆ° 1 ä¹‹é—´çš„æ¦‚ç‡ï¼Œä¸”æ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡ä¹‹å’Œä¸º 1ï¼Œæ–¹ä¾¿è§£é‡Šç»“æœã€‚ä¾‹å¦‚ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œè¾“å‡ºå¯ä»¥ç›´æ¥è¡¨ç¤ºæ¨¡å‹å¯¹å„ç±»åˆ«çš„ä¿¡å¿ƒã€‚\n",
    "> \n",
    "> - ```å¹³æ»‘å’Œæ”¾å¤§æ•ˆæœ```ï¼šSoftmax ä¸ä»…èƒ½å½’ä¸€åŒ–ï¼Œè¿˜å…·æœ‰å¹³æ»‘å’Œæ”¾å¤§æ•ˆæœã€‚è¾ƒå¤§çš„è¾“å…¥å€¼ä¼šè¢«æ”¾å¤§ï¼Œè¾ƒå°çš„è¾“å…¥å€¼ä¼šè¢«æŠ‘åˆ¶ï¼Œä»è€Œå¢å¼ºæ¨¡å‹å¯¹æœ€ä¼˜ç±»åˆ«çš„åŒºåˆ†ã€‚\n",
    ">\n",
    "> - ```æ”¯æŒå¤šåˆ†ç±»é—®é¢˜```ï¼šä¸ sigmoid ä¸åŒï¼ŒSoftmax é€‚ç”¨äºå¤šç±»åˆ«åˆ†ç±»é—®é¢˜ã€‚å®ƒå¯ä»¥è¾“å‡ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å¤„ç†å¤šåˆ†ç±»ä»»åŠ¡ã€‚\n",
    ">\n",
    "> 2. ç¥ç»ç½‘ç»œä¸ºä»€ä¹ˆå–œæ¬¢ä½¿ç”¨ Softmax\n",
    ">\n",
    "> ```åœ¨ç¥ç»ç½‘ç»œä¸­```ï¼Œç‰¹åˆ«æ˜¯åˆ†ç±»æ¨¡å‹ï¼ˆå¦‚å›¾åƒåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ï¼‰ä¸­ï¼ŒSoftmax å±‚é€šå¸¸ç”¨ä½œæœ€åä¸€å±‚è¾“å‡ºã€‚åŸå› åŒ…æ‹¬ï¼š\n",
    "> \n",
    "> - ```ä¾¿äºä¼˜åŒ–```ï¼šåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼ŒSoftmax è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒå¯ä¸çœŸå®çš„æ ‡ç­¾æ¦‚ç‡è¿›è¡Œæ¯”è¾ƒï¼Œä»è€Œè®¡ç®—äº¤å‰ç†µæŸå¤±ã€‚äº¤å‰ç†µæŸå¤±çš„æ¢¯åº¦è¾ƒä¸ºç¨³å®šï¼Œä¾¿äºæ¨¡å‹çš„ä¼˜åŒ–ã€‚\n",
    ">\n",
    "> - ```æ¦‚ç‡è§£é‡Š```ï¼šSoftmax è¾“å‡ºå¯ä»¥è§£é‡Šä¸ºâ€œæ¨¡å‹å¯¹æ¯ä¸ªç±»åˆ«çš„ä¿¡å¿ƒâ€ï¼Œä½¿å¾—è¾“å‡ºç›´è§‚å¯ç†è§£ã€‚\n",
    ">\n",
    "> - ```ä¸äº¤å‰ç†µçš„ç»“åˆ```ï¼šSoftmax ä¸äº¤å‰ç†µæŸå¤±å‡½æ•°ç»“åˆæ•ˆæœç‰¹åˆ«å¥½ï¼Œå¯ä»¥ç›´æ¥å°†æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾æ¯”è¾ƒï¼Œä»è€Œæ›´å¿«æ”¶æ•›ï¼Œæ•ˆæœæ›´å¥½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "#è®¡ç®—æœ€åçš„ç»“æœï¼Œå³ä¸Šä¸‹æ–‡å‘é‡\n",
    "query = inputs[1] # 2nd input token is the query\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imgs/PixPin_2025-07-08_14-28-30.png)\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-08_14-29-10.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®ç°å¸¦æœ‰å¯è®­ç»ƒæƒé‡çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-08_14-40-53.png)\n",
    "\n",
    "è¿™é‡Œçš„æƒé‡æŒ‡çš„æ˜¯W_q, W_k, W_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]                                                   #A\n",
    "d_in = inputs.shape[1]                                            #B\n",
    "d_out = 2                                                         #C\n",
    "\n",
    "\n",
    "#A ç¬¬äºŒä¸ªè¾“å…¥å…ƒç´ \n",
    "#B è¾“å…¥ç»´åº¦ï¼Œ d_in=3\n",
    "#C è¾“å‡ºç»´åº¦ï¼Œ d_out=2\n",
    "#è¯·æ³¨æ„ï¼Œåœ¨ GPT ç±»æ¨¡å‹ä¸­ï¼Œè¾“å…¥ç»´åº¦å’Œè¾“å‡ºç»´åº¦é€šå¸¸æ˜¯ç›¸åŒçš„ã€‚\n",
    "#ä¸è¿‡ï¼Œä¸ºäº†ä¾¿äºè¯´æ˜å’Œæ›´æ¸…æ¥šåœ°å±•ç¤ºè®¡ç®—è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨æ­¤é€‰æ‹©äº†ä¸åŒçš„è¾“å…¥ï¼ˆd_in=3ï¼‰å’Œè¾“å‡ºï¼ˆd_out=2ï¼‰ç»´åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=True)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=True)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_2 is tensor([0.4306, 1.4551], grad_fn=<SqueezeBackward4>), \n",
      "key_2 is tensor([0.4433, 1.1419], grad_fn=<SqueezeBackward4>), \n",
      "value_2 is tensor([0.3951, 1.0037], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(f\"query_2 is {query_2}, \\nkey_2 is {key_2}, \\nvalue_2 is {value_2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!NOTE]\n",
    "> æƒé‡å‚æ•° VS æ³¨æ„åŠ›æƒé‡\n",
    "> \n",
    "> è¯·æ³¨æ„ï¼Œåœ¨æƒé‡çŸ©é˜µ W ä¸­ï¼Œæœ¯è¯­â€œæƒé‡â€æ˜¯â€œæƒé‡å‚æ•°â€çš„ç¼©å†™ï¼ŒæŒ‡çš„æ˜¯ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¢«ä¼˜åŒ–çš„æ•°å€¼å‚æ•°ã€‚è¿™ä¸æ³¨æ„åŠ›æƒé‡ä¸åŒï¼Œæ³¨æ„åŠ›æƒé‡ç”¨äºç¡®å®šä¸Šä¸‹æ–‡å‘é‡å¯¹è¾“å…¥æ–‡æœ¬çš„ä¸åŒéƒ¨åˆ†çš„ä¾èµ–ç¨‹åº¦ï¼Œå³ç¥ç»ç½‘ç»œå¯¹è¾“å…¥ä¸åŒéƒ¨åˆ†çš„å…³æ³¨ç¨‹åº¦ã€‚\n",
    "> \n",
    "> æ€»ä¹‹ï¼Œæƒé‡å‚æ•°æ˜¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬å­¦ä¹ ç³»æ•°ï¼Œç”¨äºå®šä¹‰ç½‘ç»œå±‚ä¹‹é—´çš„è¿æ¥å…³ç³»ï¼Œè€Œæ³¨æ„åŠ›æƒé‡åˆ™æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€ç”Ÿæˆçš„ç‰¹å®šå€¼ï¼Œç”¨äºè¡¡é‡ä¸åŒè¯è¯­æˆ–ä½ç½®åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸­çš„é‡è¦æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524, grad_fn=<DotBackward0>)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]                                                  #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n",
    "\n",
    "#A è¯·ç‰¢è®°åœ¨Pythonä¸­ç´¢å¼•ä»0å¼€å§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "       grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imgs/PixPin_2025-07-08_14-59-40.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210], grad_fn=<SqueezeBackward4>)\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imgs/PixPin_2025-07-08_15-29-33.png)\n",
    "\n",
    "åœ¨è¿™æ®µ PyTorch ä»£ç ä¸­ï¼Œ```SelfAttention_v1``` æ˜¯ä¸€ä¸ªä» ```nn.Module``` æ´¾ç”Ÿçš„ç±»ã€‚```nn.Module``` æ˜¯ PyTorch æ¨¡å‹çš„åŸºç¡€ç»„ä»¶ï¼Œæä¾›äº†åˆ›å»ºå’Œç®¡ç†æ¨¡å‹å±‚æ‰€éœ€çš„å¿…è¦åŠŸèƒ½ã€‚\n",
    "\n",
    "```__init__``` æ–¹æ³•åˆå§‹åŒ–äº†ç”¨äºè®¡ç®—æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰çš„å¯è®­ç»ƒæƒé‡çŸ©é˜µï¼ˆ```W_query```ã€```W_key``` å’Œ ```W_value```ï¼‰ï¼Œæ¯ä¸ªçŸ©é˜µéƒ½å°†è¾“å…¥ç»´åº¦ ```d_in``` è½¬æ¢ä¸ºè¾“å‡ºç»´åº¦ ```d_out```ã€‚\n",
    "\n",
    "å‰å‘ä¼ æ’­è¿‡ç¨‹åœ¨ ```forward``` æ–¹æ³•ä¸­å®ç°ï¼Œæˆ‘ä»¬é€šè¿‡å°†æŸ¥è¯¢ï¼ˆqueryï¼‰å’Œé”®ï¼ˆkeyï¼‰ç›¸ä¹˜æ¥è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼ˆattn_scoresï¼‰ï¼Œå¹¶ä½¿ç”¨ softmax å¯¹è¿™äº›å¾—åˆ†è¿›è¡Œå½’ä¸€åŒ–ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™äº›å½’ä¸€åŒ–çš„æ³¨æ„åŠ›å¾—åˆ†å¯¹å€¼ï¼ˆvalueï¼‰åŠ æƒï¼Œç”Ÿæˆä¸Šä¸‹æ–‡å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    ###\n",
    "    ### d_in is the input dimension, d_out is the content_vector dimension\n",
    "    ###\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        attention_scores = queries @ keys.T\n",
    "        attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim = -1)\n",
    "        content_vec = attention_weights @ values\n",
    "        return content_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "self_attention_v1 = SelfAttention_v1(d_in=d_in,d_out=d_out)\n",
    "print(self_attention_v1(inputs))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imgs/PixPin_2025-07-08_15-41-12.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Linear(in_features=3, out_features=2, bias=False)\n",
      "\n",
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#  Listing 3.2 A self-attention class using PyTorch's Linear layers\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        print(f\" {self.W_key}\\n\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç»ƒä¹  3.1ï¼šæ¯”è¾ƒSelfAttention_v1å’Œ SelfAttention_v2\n",
    "\n",
    "è¯·æ³¨æ„ï¼ŒSelfAttention_v2 ä¸­çš„ nn.Linear å±‚ä½¿ç”¨äº†ä¸€ç§ä¸åŒçš„æƒé‡åˆå§‹åŒ–æ–¹å¼ï¼Œè€Œ SelfAttention_v1 åˆ™ä½¿ç”¨ nn.Parameter(torch.rand(d_in, d_out)) è¿›è¡Œåˆå§‹åŒ–ã€‚è¿™å¯¼è‡´ä¸¤ç§æœºåˆ¶ç”Ÿæˆçš„ç»“æœæœ‰æ‰€ä¸åŒã€‚ä¸ºäº†éªŒè¯ SelfAttention_v1 å’Œ SelfAttention_v2 çš„å…¶ä»–éƒ¨åˆ†æ˜¯å¦ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥å°† SelfAttention_v2 å¯¹è±¡ä¸­çš„æƒé‡çŸ©é˜µè½¬ç§»åˆ° SelfAttention_v1 ä¸­ï¼Œä»è€Œä½¿ä¸¤è€…ç”Ÿæˆç›¸åŒçš„ç»“æœã€‚\n",
    "\n",
    "ä½ çš„ä»»åŠ¡æ˜¯å°† SelfAttention_v2 å®ä¾‹ä¸­çš„æƒé‡æ­£ç¡®åˆ†é…ç»™ SelfAttention_v1 å®ä¾‹ã€‚ä¸ºæ­¤ï¼Œä½ éœ€è¦ç†è§£ä¸¤ä¸ªç‰ˆæœ¬ä¸­æƒé‡ä¹‹é—´çš„å…³ç³»ã€‚ï¼ˆæç¤ºï¼šnn.Linear å­˜å‚¨çš„æ˜¯è½¬ç½®å½¢å¼çš„æƒé‡çŸ©é˜µã€‚ï¼‰åˆ†é…å®Œæˆåï¼Œä½ åº”è¯¥èƒ½è§‚å¯Ÿåˆ°ä¸¤ä¸ªå®ä¾‹ç”Ÿæˆç›¸åŒçš„è¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: tensor(0., grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "self_attention_v1.W_key.data = sa_v2.W_key.weight.T\n",
    "self_attention_v1.W_query.data = sa_v2.W_query.weight.T\n",
    "self_attention_v1.W_value.data = sa_v2.W_value.weight.T\n",
    "\n",
    "out_v1 = self_attention_v1(inputs)\n",
    "out_v2 = sa_v2(inputs)\n",
    "\n",
    "print(\"Difference:\", torch.abs(out_v1 - out_v2).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 ä½¿ç”¨å› æœæ³¨æ„åŠ›æœºåˆ¶æ¥å±è”½åç»­è¯\n",
    "\n",
    "å› æœæ³¨æ„åŠ›ï¼ˆä¹Ÿç§°ä¸ºæ©è”½æ³¨æ„åŠ›,maskï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šçš„è‡ªæ³¨æ„åŠ›å½¢å¼ã€‚å®ƒé™åˆ¶æ¨¡å‹åœ¨å¤„ç†ä»»ä½•ç»™å®šçš„ token æ—¶ï¼Œåªèƒ½è€ƒè™‘åºåˆ—ä¸­çš„å‰ä¸€ä¸ªå’Œå½“å‰è¾“å…¥ï¼Œè€Œä¸èƒ½çœ‹åˆ°åç»­çš„å†…å®¹ã€‚è¿™ä¸æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å½¢æˆå¯¹æ¯”ï¼Œåè€…å…è®¸æ¨¡å‹åŒæ—¶è®¿é—®æ•´ä¸ªè¾“å…¥åºåˆ—ã€‚\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-08_16-19-06.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights are: \n",
      "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
      "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
      "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "context_length is 6\n",
      "\n",
      "mask_simple is \n",
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "hence the final masked_simple is \n",
      "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)                                   #A\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(f\"attn_weights are: \\n{attn_weights}\\n\")\n",
    "\n",
    "#A ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å¤ç”¨ä¸Šä¸€èŠ‚ä¸­ SelfAttention_v2 å¯¹è±¡çš„queryå’Œkeyæƒé‡çŸ©é˜µã€‚\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "print(f\"context_length is {context_length}\\n\")\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(f\"mask_simple is \\n{mask_simple}\\n\")\n",
    "masked_simple = attn_weights*mask_simple\n",
    "print(f\"hence the final masked_simple is \\n{masked_simple}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# and we want to softmax the masked_simple\n",
    "\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½’ä¸€åŒ–åæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¢«æ©ç›–çš„éƒ¨åˆ†å¹¶ä¸ä¼šå½±å“å½“å‰çš„tokenï¼Œæœªè¢«æ©ç›–çš„æ¦‚ç‡å’Œå§‹ç»ˆä¸º1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax å‡½æ•°å°†è¾“å…¥å€¼è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚å½“ä¸€è¡Œä¸­å­˜åœ¨è´Ÿæ— ç©·å€¼ï¼ˆ-âˆï¼‰æ—¶ï¼ŒSoftmax å‡½æ•°ä¼šå°†è¿™äº›å€¼è§†ä¸ºé›¶æ¦‚ç‡ã€‚ï¼ˆä»æ•°å­¦ä¸Šè®²ï¼Œè¿™æ˜¯å› ä¸º eâˆ’âˆ æ¥è¿‘äº 0ã€‚ï¼‰\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªå¯¹è§’çº¿ä»¥ä¸Šå…¨ä¸º 1 çš„æ©ç ï¼Œç„¶åå°†è¿™äº› 1 æ›¿æ¢ä¸ºè´Ÿæ— ç©·å¤§ï¼ˆ-infï¼‰å€¼ï¼Œä»è€Œå®ç°è¿™ç§æ›´é«˜æ•ˆçš„æ©ç æŠ€å·§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked: \n",
      "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
      "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
      "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
      "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weights: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(f\"masked: \\n{masked}\")\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5,dim=1)\n",
    "print(f\"attn_weights: \\n{attn_weights}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Dropout``` åœ¨æ·±åº¦å­¦ä¹ ä¸­æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œå³åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºå¿½ç•¥ä¸€äº›éšè—å±‚å•å…ƒï¼Œå®é™…ä¸Šå°†å®ƒä»¬â€œä¸¢å¼ƒâ€ã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œç¡®ä¿æ¨¡å‹ä¸ä¼šè¿‡äºä¾èµ–ä»»ä½•ç‰¹å®šçš„éšè—å±‚å•å…ƒç»„åˆã€‚éœ€è¦ç‰¹åˆ«å¼ºè°ƒçš„æ˜¯ï¼ŒDropout ä»…åœ¨```è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨```ï¼Œè®­ç»ƒ```ç»“æŸååˆ™ä¼šç¦ç”¨```ã€‚\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-08_16-44-01.png)\n",
    "\n",
    "åœ¨ä»¥ä¸‹ä»£ç ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†50%çš„ dropout ç‡ï¼Œè¿™æ„å‘³ç€å±è”½æ‰ä¸€åŠçš„æ³¨æ„åŠ›æƒé‡ã€‚ï¼ˆåœ¨åç»­ç« èŠ‚ä¸­è®­ç»ƒ GPT æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ›´ä½çš„ dropout ç‡ï¼Œæ¯”å¦‚ 0.1 æˆ– 0.2ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. ä¸ºä»€ä¹ˆè¦è¿›è¡Œç¼©æ”¾ï¼Ÿ**\n",
    "\n",
    "* **ä¿æŒè®­ç»ƒï¼æ¨ç†æ—¶æ¿€æ´»é‡ä¸€è‡´**\n",
    "  åœ¨è®­ç»ƒæ—¶ï¼ŒDropout ä¼šä»¥æ¦‚ç‡ $p$ æŠŠä¸€éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºç½®ä¸º 0ï¼Œå‰©ä¸‹çš„ç¥ç»å…ƒæŒ‰æ¯”ä¾‹æ”¾å¤§ï¼Œå¦åˆ™æ•´ä½“çš„ä¿¡å·å¼ºåº¦å°±ä¼šå˜å°ï¼Œå¯¼è‡´æ¨ç†ï¼ˆevalï¼‰æ—¶å’Œè®­ç»ƒæ—¶åˆ†å¸ƒä¸ä¸€è‡´ã€‚\n",
    "* **æ•°å­¦ä¸Šï¼šä¿æŒæœŸæœ›ä¸å˜**\n",
    "  å‡è®¾åŸå§‹æ¿€æ´»å€¼æ˜¯ $x$ï¼ŒDropout ä¸¢å¼ƒæ¦‚ç‡æ˜¯ $p$ï¼š\n",
    "\n",
    "  * â€œä¿ç•™â€æ—¶ï¼Œæˆ‘ä»¬æŠŠè¯¥ç¥ç»å…ƒçš„è¾“å‡ºè®¾ä¸º $\\displaystyle \\frac{x}{1-p}$\n",
    "  * â€œä¸¢å¼ƒâ€æ—¶ï¼Œè¾“å‡ºè®¾ä¸º 0\n",
    "    è¿™æ ·ï¼Œè®­ç»ƒæ—¶å•ä¸ªç¥ç»å…ƒçš„**æœŸæœ›è¾“å‡º**\n",
    "\n",
    "  $$\n",
    "    E[\\text{output}] = (1-p)\\times \\frac{x}{1-p} + p\\times 0 = x,\n",
    "  $$\n",
    "\n",
    "  å’ŒåŸå§‹ $x$ ä¸€è‡´ã€‚\n",
    "* **æ¨ç†æ—¶**ï¼ˆ`model.eval()`ï¼‰ï¼ŒDropout ä¸ä¸¢å¼ƒä¹Ÿä¸ç¼©æ”¾ï¼Œç›´æ¥è¾“å‡ºåŸå§‹ $x$ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "**2. Dropout å¦‚ä½•å½±å“è¿‡æ‹Ÿåˆï¼Ÿ**\n",
    "\n",
    "1. **æŠ‘åˆ¶â€œååŒé€‚åº”â€ï¼ˆCo-adaptationï¼‰**\n",
    "   å¦‚æœæ‰€æœ‰ç¥ç»å…ƒéƒ½å§‹ç»ˆåœ¨çº¿ï¼Œå®ƒä»¬æœ‰å¯èƒ½â€œäº’ç›¸ä¾èµ–â€â€”â€”æŸäº›ç‰¹å¾æ£€æµ‹å™¨åªåœ¨åˆ«çš„æ£€æµ‹å™¨å­˜åœ¨æ—¶æ‰æœ‰ç”¨ã€‚Dropout éšæœºå±è”½ï¼Œè®©æ¯ä¸ªç¥ç»å…ƒéƒ½å¿…é¡»ç‹¬ç«‹â€œæ‰›èµ·â€è§£å†³é—®é¢˜çš„è´£ä»»ï¼Œæ›´åŠ é²æ£’ã€‚\n",
    "2. **ç›¸å½“äºè®­ç»ƒäº†ä¸€ä¸ªâ€œå­ç½‘ç»œâ€é›†åˆ**\n",
    "   æ¯æ¬¡å‰å‘éƒ½éšæœºæŠ½æ‰éƒ¨åˆ†èŠ‚ç‚¹ï¼Œå°±åƒåœ¨è®­ç»ƒå¾ˆå¤šä¸åŒçš„å°ç½‘ç»œï¼ˆåŒä¸€ä¸ªå¤§ç½‘ç»œçš„ä¸åŒå­é›†ï¼‰ã€‚æœ€ç»ˆå­¦åˆ°çš„æƒé‡ï¼Œç›¸å½“äºè¿™äº›å­ç½‘ç»œè¾“å‡ºçš„å¹³å‡ï¼Œä¹Ÿç±»ä¼¼ **Bagging**ï¼Œèƒ½æ˜¾è‘—é™ä½æ–¹å·®ã€å‡è½»è¿‡æ‹Ÿåˆã€‚\n",
    "3. **å¢åŠ å™ªå£°ï¼Œæé«˜æ³›åŒ–**\n",
    "   éšæœºå±è”½ç›¸å½“äºåœ¨éšè—å±‚æ³¨å…¥å™ªå£°ï¼Œæ¨¡å‹å¿…é¡»å­¦ä¼šåœ¨å™ªå£°ä¸­æå–å¯é ä¿¡å·ï¼Œå°±ä¸ä¼šåœ¨è®­ç»ƒæ•°æ®ä¸Šâ€œæ­»è®°ç¡¬èƒŒâ€å™ªå£°ï¼ç‰¹å®šæ¨¡å¼ï¼Œä»è€Œæé«˜å¯¹æ–°æ ·æœ¬çš„æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "---\n",
    "\n",
    " ğŸ”‘ å°ç»“\n",
    "\n",
    "* **ç¼©æ”¾**ï¼šä¿è¯è®­ç»ƒï¼æ¨ç†é˜¶æ®µæ¿€æ´»åˆ†å¸ƒä¸€è‡´ï¼Œä¸ç”¨åœ¨æ¨ç†æ—¶å†é¢å¤–è°ƒæ•´æƒé‡ã€‚\n",
    "* **å‡è½»è¿‡æ‹Ÿåˆ**ï¼šè®©ç½‘ç»œæ›´å…·é²æ£’æ€§ã€é™ä½æƒé‡æ–¹å·®ã€æŠ‘åˆ¶ç‰¹å¾é—´è¿‡åº¦ååŒï¼Œä»è€Œåœ¨æ–°æ•°æ®ä¸Šè¡¨ç°æ›´å¥½ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®ç°ä¸€ä¸ªç®€æ´çš„å› æœæ³¨æ„åŠ›ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)                                              #A\n",
    "\n",
    "#A 2ä¸ªè¾“å…¥ï¼Œæ¯ä¸ªè¾“å…¥æœ‰6ä¸ªtokenï¼Œæ¯ä¸ªtokençš„åµŒå…¥ç»´åº¦ä¸º3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in,d_out,bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length),diagonal=1))          #B\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape                               #C\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        # D. åŸåœ°æŠŠâ€œæœªæ¥â€ä½ç½®çš„åˆ†æ•°ç½®ä¸º -infï¼ˆsoftmax åæƒé‡â‰ˆ0ï¼‰ï¼Œä¿è¯ causalã€‚\n",
    "        #    masked_fill_ ä¼šç›´æ¥åœ¨ attn_scores çš„å†…å­˜ä¸Šä¿®æ”¹ï¼Œé¿å…å¤šä½™æ‹·è´ã€‚\n",
    "        attn_scores.masked_fill_(                                 #D\n",
    "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "#B register_buffer è°ƒç”¨ä¹Ÿæ˜¯æ–°æ·»åŠ çš„å†…å®¹ï¼ˆåç»­å†…å®¹ä¼šæä¾›æ›´å¤šç›¸å…³ä¿¡æ¯ï¼‰\n",
    "#C æˆ‘ä»¬äº¤æ¢ç¬¬ 1 å’Œç¬¬ 2 ä¸ªç»´åº¦ï¼ŒåŒæ—¶ä¿æŒæ‰¹æ¬¡ç»´åº¦åœ¨ç¬¬1ä¸ªä½ç½®ï¼ˆç´¢å¼•0ï¼‰\n",
    "#D åœ¨ PyTorch ä¸­ï¼Œå¸¦æœ‰ä¸‹åˆ’çº¿åç¼€çš„æ“ä½œä¼šåœ¨åŸæœ‰å†…å­˜ç©ºé—´æ‰§è¡Œï¼Œç›´æ¥ä¿®æ”¹å˜é‡æœ¬èº«ï¼Œä»è€Œé¿å…ä¸å¿…è¦çš„å†…å­˜æ‹·è´"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è™½ç„¶æ–°å¢çš„ä»£ç è¡Œä¸ä¹‹å‰ç« èŠ‚ä»‹ç»çš„å†…å®¹åŸºæœ¬ä¸€è‡´ï¼Œä½†æˆ‘ä»¬ç°åœ¨åœ¨ __init__ æ–¹æ³•ä¸­æ·»åŠ äº† self.register_buffer() çš„è°ƒç”¨ã€‚register_buffer åœ¨ PyTorch ä¸­å¹¶éæ‰€æœ‰æƒ…å†µä¸‹éƒ½å¿…é¡»ä½¿ç”¨ï¼Œä½†åœ¨è¿™é‡Œæœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­ä½¿ç”¨ CausalAttention ç±»æ—¶ï¼Œbuffer ä¼šè‡ªåŠ¨éšæ¨¡å‹è¿ç§»åˆ°åˆé€‚çš„è®¾å¤‡ï¼ˆCPU æˆ– GPUï¼‰ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬æ— éœ€æ‰‹åŠ¨ç¡®ä¿è¿™äº›å¼ é‡ä¸æ¨¡å‹å‚æ•°åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œä»è€Œé¿å…è®¾å¤‡ä¸åŒ¹é…é”™è¯¯ã€‚\n",
    "\n",
    "keys.transpose(1, 2)çš„ä½œç”¨ï¼škeys çš„å½¢çŠ¶æ˜¯ (batch_size, seq_len, d_out)ï¼Œè€Œæˆ‘ä»¬è¦åšçš„æ˜¯æŠŠæ¯ä¸ªä½ç½®çš„ query å‘é‡å’Œæ‰€æœ‰ä½ç½®çš„ key å‘é‡åšç‚¹ç§¯ï¼Œä¹Ÿå°±æ˜¯æŠŠ key çŸ©é˜µâ€œè½¬ç½®â€æˆ (batch_size, d_out, seq_len)ï¼Œæ‰èƒ½ç»§ç»­è®¡ç®—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 ä»å•å¤´æ³¨æ„åŠ›æ‰©å±•åˆ°å¤šå¤´æ³¨æ„åŠ›\n",
    "\n",
    "åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå®ç°å¤šå¤´æ³¨æ„åŠ›éœ€è¦åˆ›å»ºå¤šä¸ªè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å®ä¾‹ï¼ˆåœ¨ 3.4.1 èŠ‚çš„å›¾ 3.18 ä¸­å·²æœ‰å±•ç¤ºï¼‰ï¼Œæ¯ä¸ªå®ä¾‹éƒ½å…·æœ‰ç‹¬ç«‹çš„æƒé‡ï¼Œç„¶åå°†å®ƒä»¬çš„è¾“å‡ºåˆå¹¶ã€‚å¤šä¸ªè‡ªæ³¨æ„åŠ›æœºåˆ¶å®ä¾‹çš„åº”ç”¨å±äºè®¡ç®—å¯†é›†å‹ï¼ˆCPUå¯†é›†å‹ï¼‰æ“ä½œï¼Œä½†å®ƒå¯¹äºè¯†åˆ«å¤æ‚æ¨¡å¼è‡³å…³é‡è¦ï¼Œè¿™æ˜¯åŸºäº Transformer çš„å¤§è¯­è¨€æ¨¡å‹æ‰€æ“…é•¿çš„èƒ½åŠ›ä¹‹ä¸€ã€‚\n",
    "\n",
    "![Alt text](imgs/PixPin_2025-07-08_17-07-15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imgs/PixPin_2025-07-08_17-10-02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "print(context_length)\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”±æ­¤ç”Ÿæˆçš„ context_vecs å¼ é‡çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯ 2ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰ä¸¤ä¸ªè¾“å…¥æ–‡æœ¬ï¼ˆè¾“å…¥æ–‡æœ¬è¢«å¤åˆ¶ï¼Œå› æ­¤å®ƒä»¬çš„ä¸Šä¸‹æ–‡å‘é‡å®Œå…¨ç›¸åŒï¼‰ã€‚ç¬¬äºŒä¸ªç»´åº¦å¯¹åº”æ¯ä¸ªè¾“å…¥ä¸­çš„ 6 ä¸ª tokenã€‚ç¬¬ä¸‰ä¸ªç»´åº¦å¯¹åº”æ¯ä¸ª token çš„ 4 ç»´åµŒå…¥å‘é‡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
